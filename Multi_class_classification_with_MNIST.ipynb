{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-class classification with MNIST.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Multi-Class Image Classification\n",
        "MNIST dataset - This is a multi-class classification problem with 10 output classes, one for each digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuKlphuImFSN"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "  * Understand the MNIST dataset.\n",
        "  * Create and tune a deep neural network for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxj8yVh4mFl5"
      },
      "source": [
        "## MNIST Dataset\n",
        "  \n",
        "* The MNIST training set contains 60,000 examples.\n",
        "* The MNIST test set contains 10,000 examples.\n",
        "\n",
        "Each example contains a pixel map showing how a person wrote a digit. For example, the following images shows how a person wrote the digit `1` and how that digit might be represented in a 14x14 pixel map (after the input data is normalized).\n",
        "\n",
        "![image](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
        "\n",
        "Each example in the MNIST dataset consists of:\n",
        "\n",
        "* A label specified by a human.  Each label must be an integer from 0 to 9.  For example, in the preceding image, the rater would almost certainly assign the label `1` to the example.\n",
        "* A 28x28 pixel map, where each pixel is an integer between 0 and 255. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent various shades of gray.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX_umRMMsa3z"
      },
      "source": [
        "## Use the right version of TensorFlow\n",
        "\n",
        "The following hidden code cell ensures that the Colab will run on TensorFlow 2.X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM75uNH-sTv2"
      },
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "source": [
        "#Import relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Just like `sklearn`, `tf.keras`\n",
        "\n",
        "* Loads both the training set and the test set.\n",
        "* Separates each set into features and labels.\n",
        "\n",
        "The relevant convenience function for MNIST is called `mnist.load_data()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfQkr3hxJGXU"
      },
      "source": [
        "The `mnist.load_data()` returned four separate values:\n",
        "\n",
        "* `x_train` contains the training set's features.\n",
        "* `y_train` contains the training set's labels.\n",
        "* `x_test` contains the test set's features.\n",
        "* `y_test` contains the test set's labels.\n",
        "\n",
        "The MNIST .csv training set is already shuffled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNr2TtOB1ePq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71vsSUM7pdmu"
      },
      "source": [
        "## View the dataset\n",
        "\n",
        "MNIST does not contain column names. It is best to think of `x_train` and `x_test` as three-dimensional NumPy arrays:  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoOhpjkeCL8Q"
      },
      "source": [
        "# Output example #1 of the training set. this is probably the digit 0\n",
        "x_train[1]\n",
        "#or \n",
        "#plt.imshow(x_train[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxtSd87O2EM9"
      },
      "source": [
        "#check the digit label for example #1\n",
        "y_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldP-5z1B2vL"
      },
      "source": [
        "##Normalize feature values\n",
        "\n",
        "Normalize the x_train and x_test dataset to bring values between 0 and 1.0. Store the values in `x_train_normalized` and `x_test_normalized`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQljE-wizDw"
      },
      "source": [
        "x_train_normalized = x_train/255\n",
        "x_test_normalized = x_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWRF6CStuNA"
      },
      "source": [
        "## Define a plotting function\n",
        "\n",
        "The following function plots an accuracy curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "source": [
        "#@title Define the plotting function\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Loaded the plot_curve function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Create a deep neural net model\n",
        "\n",
        "Specify:\n",
        "* The number of `layers` in the deep neural net.\n",
        "* The number of `nodes` in each layer.\n",
        "* Any `regularization` layers.\n",
        "\n",
        "The activation function of the output layer is `softmax`, which will yield 10 different outputs for each example. Each of the 10 outputs provides the probability that the input example is a certain digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iXZBT-IF7_"
      },
      "source": [
        "#instantiate the model\n",
        "model = None\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#The features are stored in a two-dimensional 28X28 array. Flatten that two-dimensional array into a a one-dimensional 784-element array.\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "#Define the first hidden layer.   \n",
        "model.add(tf.keras.layers.Dense(units=32, activation='relu',kernel_regularizer=tf.keras.regularizers.l1(0.04)))\n",
        "\n",
        "#For dropout regularization layer. \n",
        "#model.add(tf.keras.layers.Dropout(rate=0.2)\n",
        "\n",
        "# Define the output layer. The units parameter is set to 10 because the model must choose among 10 possible output values (representing the digits from 0 to 9, inclusive).\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "#The loss function for multi-class classification is different than the loss function for binary classification\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.003),\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpjkZsx4JaI4"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit(x=x_train_normalized, y=y_train, batch_size=4000,\n",
        "                      epochs=50, shuffle=True, \n",
        "                      validation_split=0.2, verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQiXLL5gKzsX"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.plot(history.history['accuracy'] , label='Accuracy')\n",
        "print(\"Accuracy:\", history.history['accuracy'][-1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5IKmk7D49_n"
      },
      "source": [
        "Optimize the model\n",
        "\n",
        "Experiment with the following:\n",
        "\n",
        "* number of hidden layers \n",
        "* number of nodes in each layer\n",
        "* dropout regularization rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiV-NzAoMqyy"
      },
      "source": [
        "\n",
        "#### Adding more nodes (at least until 256 nodes) to the first hidden layer can improve accuracy. Adding a second hidden layer also improves accuracy. When the model contains a lot of nodes, the model overfits unless the dropout rate is at least 0.5. \n"
      ]
    }
  ]
}